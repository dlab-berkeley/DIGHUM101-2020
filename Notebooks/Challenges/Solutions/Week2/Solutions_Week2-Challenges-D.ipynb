{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1: spaCy Linguistic Features\n",
    "\n",
    "Use spaCy to create a [linguistic features table](https://spacy.io/usage/linguistic-features). [Part of speech tagging](https://stackabuse.com/python-for-nlp-parts-of-speech-tagging-and-named-entity-recognition/) is a means to \"[assign] parts of speech to individual words in a sentence\". \n",
    "\n",
    "What are the [ten parts of speech](https://www.theclassroom.com/10-parts-speech-8344653.html)?\n",
    "\n",
    "[Click here to view part of speech tagging abbreviations](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large pretrained model\n",
    "# (load sm or md if it crashes your computer)\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'be', 'think', 'if', 'off', 'the', 'top', 'of', '-PRON-', 'head', '-PRON-', 'be', 'aware', 'of', 'a', 'generalizable', 'comprehension', 'to', 'quickly', 'stem', 'all', 'word', 'in', 'a', 'list', 'of', 'token', 'and', 'how', 'to', 'quickly', 'write', 'up', 'a', 'one', '-', 'minute', 'example', '?', 'this', 'will', 'be', 'really', 'useful', 'for', 'student', 'interested', 'in', 'text', 'preprocessing', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define our function \n",
    "def lemmatize(tokens):\n",
    "    \"\"\"Return the lemmas for each word in `tokens`.\"\"\"\n",
    "    # spacy models operate on strings not lists, so we turn the tokens back into\n",
    "    # a string of words\n",
    "    words = ' '.join(tokens)\n",
    "    # this line does all sorts of processing, including the lemmatization.\n",
    "    # `doc` will be like a list of tokens that we can iterate over\n",
    "    doc = nlp(words)\n",
    "    # each token in `doc` holds information about that token. The `lemma_`\n",
    "    # attribute holds the lemma of that token represented as a string. For\n",
    "    # performance reasons, the `lemma` (without the trailing underscore) holds\n",
    "    # an integer representation of the token, that we'll rarely ever need.\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "tokens = ('''I was thinking if off the top of your head you are aware of a \n",
    "generalizable comprehension to quickly stem all words in a list of tokens and \n",
    "how to quickly write up a one-minute example? This will be really useful for \n",
    "students interested in text preprocessing.''').split()\n",
    "\n",
    "lemmas = lemmatize(tokens)\n",
    "# Notice that spacy lemmatizes pronouns (e.g. \"you\", \"I\", \"your\") in a funny way '-PRON-'.\n",
    "# It just tells us that they are pronouns, rather than giving us something like\n",
    "# \"your\" -> \"you\".\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example: \n",
    "    \n",
    "Now that our function is defined, let's try it on another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', ',', 'jump', ',', 'run', ',', 'quicke', ',', 'eat', ',', 'quickly', '-', 'all', 'in', 'a', 'day', 'work', 'for', '-PRON-', ',', '-PRON-', ',', '-PRON-', ',', 'and', '-PRON-', '!']\n"
     ]
    }
   ],
   "source": [
    "# Does this cell work correctly? Does it give us any extraneous information?\n",
    "tokens2 = (\"Thinking, jumping, running, quicking, eating, quickly - all in a days work for me, you, she, and he!\").split()\n",
    "\n",
    "lemmas2 = lemmatize(tokens2)\n",
    "print(lemmas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking jumping running quicking eating quickly  all in a days work for me you she and he\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation ...\n",
    "sentence = \"Thinking, jumping, running, quicking, eating, quickly - all in a days work for me, you, she, and he!\"\n",
    "\n",
    "for char in punctuation:\n",
    "    sentence = sentence.replace(char, \"\")\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', 'jump', 'run', 'quicke', 'eat', 'quickly', 'all', 'in', 'a', 'day', 'work', 'for', '-PRON-', '-PRON-', '-PRON-', 'and', '-PRON-']\n"
     ]
    }
   ],
   "source": [
    "# Re-run the lemmatizer!\n",
    "# Why is \"quickly\" not lemmatized? (Because it is an adverb perhaps? Is 'quicking' a word?)\n",
    "tokens3 = sentence.split()\n",
    "\n",
    "lemmas3 = lemmatize(tokens3)\n",
    "print(lemmas3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to data frame\n",
    "\n",
    "Convert the linguistic features table to a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>shape</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>is_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thinking</td>\n",
       "      <td>think</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jumping</td>\n",
       "      <td>jump</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quicking</td>\n",
       "      <td>quicke</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eating</td>\n",
       "      <td>eat</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quickly</td>\n",
       "      <td>quickly</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>advmod</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>advmod</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>x</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>days</td>\n",
       "      <td>day</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>pobj</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>work</td>\n",
       "      <td>work</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>me</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pobj</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>you</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>dobj</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>she</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>appos</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>cc</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>he</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text    lemma    pos  tag     dep  shape  is_alpha  is_stop\n",
       "0   Thinking    think   VERB  VBG   nsubj  Xxxxx      True    False\n",
       "1    jumping     jump   VERB  VBG   ccomp   xxxx      True    False\n",
       "2    running      run   VERB  VBG   xcomp   xxxx      True    False\n",
       "3   quicking   quicke   VERB  VBG   xcomp   xxxx      True    False\n",
       "4     eating      eat   VERB  VBG   xcomp   xxxx      True    False\n",
       "5    quickly  quickly    ADV   RB  advmod   xxxx      True    False\n",
       "6        all      all    DET   DT  advmod    xxx      True     True\n",
       "7         in       in    ADP   IN    prep     xx      True     True\n",
       "8          a        a    DET   DT     det      x      True     True\n",
       "9       days      day   NOUN  NNS    pobj   xxxx      True    False\n",
       "10      work     work   NOUN   NN    ROOT   xxxx      True    False\n",
       "11       for      for    ADP   IN    prep    xxx      True     True\n",
       "12        me   -PRON-   PRON  PRP    pobj     xx      True     True\n",
       "13       you   -PRON-   PRON  PRP    dobj    xxx      True     True\n",
       "14       she   -PRON-   PRON  PRP   appos    xxx      True     True\n",
       "15       and      and  CCONJ   CC      cc    xxx      True     True\n",
       "16        he   -PRON-   PRON  PRP    ROOT     xx      True     True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define doc\n",
    "doc = nlp(\" \".join(tokens3))\n",
    "\n",
    "d = []\n",
    "for token in doc:\n",
    "    d.append((token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop))\n",
    "\n",
    "out = pd.DataFrame(d, columns=(\"text\", \"lemma\", \"pos\", \"tag\", \"dep\", \"shape\", \n",
    "                               \"is_alpha\", \"is_stop\"))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2: Repeat\n",
    "\n",
    "Repeat Challenge 1 with a text of your choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3: Context\n",
    "\n",
    "If you are doing anything that involves text for your individual final project, be sure to start thinking about **_why_** you might want to use n-grams, word2vec, or BERT instead of single-word tokenization. Start brainstorming now even if it all doesn't make total yet!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
