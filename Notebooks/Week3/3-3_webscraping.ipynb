{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with BeautifulSoup\n",
    "\n",
    "We're going to scrape some information from Wikipedia, which has a simple page layout with a consistent template.\n",
    "\n",
    "For web scraping we're going to need two libraries: [requests](https://requests.readthedocs.io/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). BeautifulSoup is what we use to actually navigate and parse the page that we're scraping. We'll import the `time` library too. This will allow us to `time.sleep(5)` so that we don't overload anyone's servers. \n",
    "\n",
    "We will talk about HTML and CSS! [What are HTML and CSS?](https://html.com/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this exercise, we will scrape all the citations on the Wikipedia \"Data Science\" page\n",
    "\n",
    "First we use requests to make a `.get` request to the page. Let's see what's on the [Data science](https://en.wikipedia.org/wiki/Data_science) Wikipedia page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://en.wikipedia.org/wiki/Data_science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an .html object, but there is no .html method in the requests library, but BeautifulSoup will help us get there. First, extract the html string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = r.text\n",
    "source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! If you visit the Data Science Wikipedia page, right click with your mouse and click \"View source\" - it's the same thing! Now we use BeatifulSoup to convert it into a soup class object that makes navigating the HTML tree much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source, 'html5lib')\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the `.prettify()` method to look at the HTML, and even get a slice of it. Let's take a look at what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use BeautifulSoup functions to find things on a page, such as:\n",
    "\n",
    "1. HTML tags\n",
    "2. HTML Attributes\n",
    "3. CSS Selectors\n",
    "\n",
    "Let's search first for **HTML tags**. \n",
    "\n",
    "The function `find_all` searches the `soup` tree to find all the elements with a particular HTML tag, and returns a list of all those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `.find_all()` method is used so frequently, there is a shortcut for it. You can just treat the soup object itself as a function, and pass it the tag you're looking for as an argument.\n",
    "\n",
    "So `soup.find_all('a')` is the same as `soup('a')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a') == soup('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that `.soup('a')` returned a lot of elements, most of which we might not want. One way to narrow down our search is to specify that we're only looking for elements that have a certain CSS class. Alternatively we can use the `.select()` method. We pass the method an argument that consists of the tag and the CSS class separated by a period. We can grab all the links in the navigation box in the upper right with the following CSS selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.select(\"table.vertical-navbox.nowraplinks.plainlist a\")\n",
    "soup.select(\"table.vertical-navbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're looking for a quick crash course in developer tools, check out this [YouTube video](https://www.youtube.com/watch?v=FQKvro1Wz-E).\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/FQKvro1Wz-E/0.jpg)](https://www.youtube.com/watch?v=FQKvro1Wz-E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the first citation\n",
    "\n",
    "Let's find all the places in the text where there is a citation, along with the references themselves. Using the `.select()` method, find all the elements in the page that belong to the \"reference-text\" class.\n",
    "\n",
    "****\n",
    "\n",
    "Once we identify elements, we want to access the information in a certain element. This usually means two things:\n",
    "\n",
    "1. Text\n",
    "2. Attributes\n",
    "\n",
    "Getting the text inside an element is easy. All we have to do is use the \"text\" member of a \"tag\" object. Let's look at the first citation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_citation = soup.select(\"span.reference-text\")[0]\n",
    "first_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out its type\n",
    "print(type(first_citation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a tag! Which means it has a `text` member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an attribute - not a method :D\n",
    "first_citation.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us the text of the citation. But we can also dig deeper into the tag to get other information that's contained there.\n",
    "\n",
    "If we want to get the link to this citation, we just have to navigate to it. We can again find whatever `a` elements are in this tag, just like we did for the soup object as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the \"a\" elements\n",
    "print(first_citation(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this returns a list. In this case the link is located in the first item. We can get that with indexing :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first one\n",
    "print(first_citation(\"a\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is also a tag. Now let's use the `.attrs` attribute to see the tag's attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_citation(\"a\")[0].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that it looks a lot like a dictionary, so we can index it as such. Since we want the link, we can use the `href` attribute like a dictionary key to get the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_citation(\"a\")[0]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get all the links contained in the references and add them to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make accumulator list\n",
    "refs_list = []\n",
    "\n",
    "# start at the endnotes\n",
    "references = soup.select(\"span.reference-text\")\n",
    "\n",
    "# loop through references\n",
    "for ref in references:\n",
    "    if ref(\"a\") != []:  # ignore the references without links\n",
    "        \n",
    "        a_element = ref(\"a\")[0]\n",
    "        link = a_element['href']\n",
    "        \n",
    "        refs_list.append(link)\n",
    "\n",
    "# get rid of links to wiki articles\n",
    "refs_list = [ref for ref in refs_list if not ref.startswith('/wiki')]\n",
    "\n",
    "refs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to data frame\n",
    "citations_df = pd.DataFrame(refs_list, columns = [\"Citation\"])\n",
    "citations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to .csv\n",
    "citations_df.to_csv(\"citations.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
